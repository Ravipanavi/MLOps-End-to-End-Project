

# ğŸš€ Vehicle Insurance Prediction â€” End-to-End MLOps Project Production-grade MLOps system built using 

## MongoDB Atlas

## FastAPI

## Scikit-Learn

## AWS S3, ECR, EKS

## Docker

## GitHub Actions

## Kubernetes

## Prometheus + Grafana

## SMOTE, Feature Engineering, Model Registry, CI/CD

# ğŸ§  Problem Statement

Predict whether a customer will purchase vehicle insurance using demographic and historical data.

The system implements a fully automated MLOps pipeline, from data ingestion to production deployment and monitoring.

# ğŸ“Œ Architecture (High-Level)

âœ”ï¸ Data Source â†’ MongoDB Atlas
âœ”ï¸ Training Pipeline â†’ Python + Scikit-Learn
âœ”ï¸ Model Registry â†’ AWS S3
âœ”ï¸ CI/CD â†’ GitHub Actions
âœ”ï¸ Deployment â†’ AWS EKS (Kubernetes)
âœ”ï¸ Observability â†’ Prometheus + Grafana

Automatic model retraining & version comparison included!

# ğŸ“‚ Project Setup
1ï¸âƒ£ Create project structure
python template.py

2ï¸âƒ£ Install local package

Add modules in:

setup.py

pyproject.toml

Reference: crashcourse.txt

3ï¸âƒ£ Create & activate virtual environment
conda create -n vehicle python=3.10 -y
conda activate vehicle
pip install -r requirements.txt
pip list

ğŸ—ƒï¸ MongoDB Atlas Setup

Sign up â†’ Create project

Deploy free tier cluster (M0)

Create DB user + password

Add IP access: 0.0.0.0/0

Copy connection string (Python, v4.6+)

Create notebook & push dataset to MongoDB

Verify data in Collections â†’ Browse data

ğŸ§¾ Logging & Exception Handling

Create logger.py

Create exception.py

Test using:

python demo.py

# ğŸ“Š Data Pipeline Components
âœ”ï¸ 1. Data Ingestion

Define config: constants/__init__.py

Write DB connection: configuration/mongo_db_connection.py

Data access: data_access/proj1_data.py

Entity configs: entity/config_entity.py

Artifacts: entity/artifact_entity.py

Set MongoDB URL:

Bash
export MONGODB_URL="mongodb+srv://...."

PowerShell
$env:MONGODB_URL="mongodb+srv://...."

# âœ”ï¸ 2. Data Validation

Add schema.yaml

Implement validation code in data_validation component

Validate column types, missing fields, schema mismatch

# âœ”ï¸ 3. Data Transformation

Feature encoding

One-hot encoding

Scaling

SMOTE-ENN imbalance handling

Save preprocessor.pkl

# âœ”ï¸ 4. Model Training

Build model (RandomForestClassifier)

Compute metrics:

F1

Precision

Recall

Save model.pkl

# â˜ï¸ AWS Integration (Model Registry + Push)
Create IAM user (CLI access)

Create access key

Export to terminal:

export AWS_ACCESS_KEY_ID="xxx"
export AWS_SECRET_ACCESS_KEY="yyy"

Create S3 bucket
mlops-project-model-mlopsproj

Code required

aws_connection.py

s3_estimator.py

# âœ”ï¸ 5. Model Evaluation

Load production model from S3

Compare new vs old model

If new model F1 > old model F1 â†’ push to S3 automatically

Threshold:

MODEL_EVALUATION_CHANGED_THRESHOLD_SCORE = 0.02

# âœ”ï¸ 6. Model Pusher

Upload model to S3 registry

Maintain versioning

# âœ”ï¸ 7. Prediction Pipeline

REST API using FastAPI

Swagger UI enabled

Load model + preprocessor from S3

# ğŸ³ CI/CD â€” Docker + GitHub Actions
YAML contains:

Build Docker image

Login to ECR

Push image to ECR

# Deploy to EKS

GitHub Secrets
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
AWS_DEFAULT_REGION
ECR_REPO

# â˜¸ï¸ AWS EKS Deployment
Install CLI
kubectl
eksctl

Create EKS cluster
eksctl create cluster \
--name vehicle-insurance-cluster \
--region us-east-1 \
--node-type t2.medium \
--nodes 2

Deploy Kubernetes manifests
kubectl apply -f deployment.yaml
kubectl apply -f service.yaml

# ğŸ“ˆ Observability (Production Monitoring)

âœ”ï¸ Prometheus â€” metrics scraping
âœ”ï¸ Grafana â€” dashboards
âœ”ï¸ Helm â€” deployment automation

# Monitor:

API latency

Pod usage

Model prediction distribution

Error rate


# ğŸ§¹ Cleanup & Tear-Down

Delete EKS cluster

Delete ECR repo

Delete S3 bucket

Delete IAM users

Delete MongoDB cluster

Remove GitHub secrets

eksctl delete cluster --name vehicle-insurance-cluster

# ğŸ“Œ Repository

ğŸ”— https://github.com/Ravipanavi/MLOps-End-to-End-Project

â­ï¸ Key Features

âœ”ï¸ Fully automated ML training pipeline
âœ”ï¸ Model registry using AWS S3
âœ”ï¸ Auto model comparison (A/B)
âœ”ï¸ CI/CD deployment on Kubernetes
âœ”ï¸ 100% production-ready
âœ”ï¸ Scalable API using FastAPI
âœ”ï¸ Live monitoring dashboards
